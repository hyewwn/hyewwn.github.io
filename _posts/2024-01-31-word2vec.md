---
layout: post
title: '[Paper Review] Efficient Estimation of Word Representations in Vector Space' # Title of the page
hide_title: false # [Opt] Hide the title when displaying the post, but shown in lists of posts
#feature-img: "assets/img/sample.png"              # [Opt] Add a feature-image to the post
#thumbnail: "assets/img/thumbnails/sample-th.png"  # [Opt] Add a thumbnail image on blog view
#color: rgb(80,140,22)                             # [Opt] Add the specified colour as feature image, and change link colors in post
# position: 1 # [Opt] Set position on the menu navigation bar
tags: [NLP, Word Embedding] # [Opt] Add tags to the page
# author:
categories: NLP
---

이 논문은 NLP 분야에 처음 입문하게 되면 가장 먼저 읽게되는 논문입니다. 단어를 일정 길이의 벡터로 변환시키는 Word Embedding의 한 방법을 다루고 있습니다.

이 논문에서 제안하고 있는 모델은 **_Word2Vec_** 이며, Word2Vec은 학습 방식으로 CBOW와 Skip-gram을 가집니다.

---

## Abstract

Propose two novel model architecture: for **vector representations of words** from large data sets.

Quality is measured in a **word similarity task**. (Syntatic and semantic word similarities)

Large improvements in **accuracy** at much **lower computational cost**.

## Goals of the Paper

**거대한 단어 데이터 세트에서 높은 퀄리티의 word-vector를 학습할 수 있는 방법에 대해 소개하고자 한다.** 이전에 제시된 어떠한 architecture도 몇 억 개의 단어들을 50에서 100 정도의 적절한 차원의 word-vector로 학습되지 못하였다.

Vector representations의 퀄리티를 측정하기 위해 최근 제시된 기술들을 사용하였는데, 이때 유사 단어들은 서로 가까울 뿐 아니라 다각도의 유사성을 지닐 것이라는 예상을 바탕으로 한다. (ex. 비슷한 어미를 가진 단어 등.)

다소 놀랍게도, 단어들의 유사성은 단순한 문법적 규칙 이상으로 나타난다. 예를 들어 vector(’King’) - vector(’Man’) + vector(’Woman’)을 계산하면 Queen이라는 단어가 도출된다.

**우리는 단어들 간의 선형적 규칙들을 보존하는 새로운 model architecture를 개발함으로써 이런 vector operation의 정확도를 최대화하고자 한다.**

### **Computational Complexity**

Computational complexity of a model은 **모델을 완전히 학습시키기 위해 필요한 파라미터의 수**로 정의한다.

**우리는 정확도를 최대화하고, computational complexity를 최소화하도록 할 것이다.**

모델들을 비교하기 위한 training complexity는 다음과 같은 식으로 나타낼 수 있다.

> **O = E \* T \* Q**

이때 E는 training epoch를, T는 training set에 포함된 단어의 수를, 그리고 Q는 모델에 따라 결정되는 인자이다. 모든 모델들은 Stochastic gradient descent와 backpropagation으로 학습된다.

## New Log-linear Models

여기서는 2개의 새로운 model architectures를 소개함으로써 distributed representations of words의 computational complexity를 최소화하고자 한다.

NNLM, RNNLM 모델은 비선형 은닉층으로 인해 높은 complexity가 유발되었기에 우리는 더 간단한 모델들을 통해 데이터를 더욱 효과적으로 학습시키고자 한다.

### Continuous Bag-of-Words Model (CBOW)

**이 모델은 feedforward NNLM과 비슷한 구조를 가지고 있으나, 비선형 은닉층이 제거되었고 투사층을 모든 단어들이 공유한다**. 이 모델에서 과거 단어들의 순서는 투사에 영향을 주지 않으므로 bag-of-word model이라 하였으며, 미래의 단어들도 사용하기 때문에 Continuous Bag-of-Words Model로 명명하였다.

<img src="/assets/img/pexels/cbow.png" width="50%">

**CBOW 모델은 한 단어 앞 뒤의 단어들을 사용하여 중간의 단어를 예측하는 모델이다. 이때 NNLM에서와 같이 투입층과 투사층 사이에 사용되는 weight matrix가 모든 단어들에 공유된다는 점에 주목해야한다.**

이 모델의 Training complexity는 다음과 같이 나타낼 수 있다.

> **Q = N \* D + D \* log2(V)**

### Continuous Skip-gram Model

두 번째 architecture는 CBOW와 비슷하나, **현재 단어를 예측하는 것이 아니라 현재 단어를 바탕으로 같은 문장의 다른 단어들을 예측하는 모델이다**. 어떤 단어를 input으로 넣으면 continuous projection layer를 통해 주변 일정 범위에 속하는 단어들을 예측한다.

<img src="/assets/img/pexels/skipgram.png" width="50%">

우리는 더 넓은 범위를 지정할수록 word vector의 퀄리티가 향상되는 것을 확인하였으나, 이는 동시에 computational complexity도 증가시킨다. 따라서 **우리는 가까운 단어에 더 높은 weight를, 멀리있는 단어에 낮은 weight를 부여하여 샘플링하는 방법을 사용하였다**.

이 모델의 Training Complexity는 다음과 같다.

> **Q = C \* (D + D \* log2(V))**

이때 C는 단어들의 최대 거리를 의미한다.

### Comparision of Model Architecture

<img src="/assets/img/pexels/word2vec_result.png" width="80%">

CBOW와 Skip-gram 모두 높은 정확도를 NNLM보다 높은 정확도를 보이고 있다.

## Conclusion

CBOW와 skip-gram이라는 새로운 word embedding 학습 방법을 제안했다. 기존의 여러 model들에 비해 연산량이 현저히 적고, 간단한 model임에도 매우 높은 성능을 보였다.

---

> Word Embedding의 결과로 단어 간의 의미관계가 다차원 벡터공간 속 좌표로 치환된다는 점은 여전히 흥미로운 부분입니다. 그러나 쉽게 짐작할 수 있다시피 자연어는 수 많은 단어로 이루어져 있기에 Computational complexity가 상당히 높습니다. Word2Vec은 자연어 처리 태스크에서 중요한 영역인 Word Embedding의 속도와 정확도를 모두 크게 향상 시켰다는 점에서 큰 의미를 갖는 것 같습니다.
