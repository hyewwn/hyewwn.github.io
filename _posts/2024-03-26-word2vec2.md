---
layout: post
title: '[Paper Review] Distributed Representations of Words and Phrases and their Compositionality' # Title of the page
hide_title: false # [Opt] Hide the title when displaying the post, but shown in lists of posts
#feature-img: "assets/img/sample.png"              # [Opt] Add a feature-image to the post
#thumbnail: "assets/img/thumbnails/sample-th.png"  # [Opt] Add a thumbnail image on blog view
#color: rgb(80,140,22)                             # [Opt] Add the specified colour as feature image, and change link colors in post
# position: 1 # [Opt] Set position on the menu navigation bar
tags: [NLP, Word Embedding] # [Opt] Add tags to the page
# author:
categories: NLP
---

앞서 Word2Vec 논문을 다루었는데요, 이번 논문은 앞선 Word2Vec 논문을 더욱 발전시키는 것을 목표로 하고 있습니다. 따라서 앞선 Word2Vec 논문을 먼저 보고 오시는 것을 추천드립니다.

즉, 본 논문의 목표는 **Skip-gram model을 이용하여 distributed vector representations를 학습시킬 때, 학습 시간과 학습 퀄리티를 더욱 향상시키자** 입니다.

---

## Abstract

**Continuous Skip-gram** model is an efficient model for learning high-quality **distributed vector representations**. 

In this paper we present several extensions that improve both the quality of the vectors and the training speed.

By **subsampling of the frequent words**, we obtain significant speedup and also learn more regular word representations. Also, we described simple alternative to the hierarchical softmax called **negative sampling**.

**An inherent limitation of word representations** is their **indifference to word order and their inability to represent idiomatic phrases**. We present a simple method for finding phrases in text.

## Goals of the Paper

앞서 언급했듯이 본 논문의 목표는 Word Representation의 Quality와 Speed를 모두 향상시키는 것에 있다. 저자들은 이를 위해 총 3가지 방법을 제시하고 있다.

먼저 Quality와 Speed를 모두 향상시킬 수 있는 방법으로 **Negative Sampling**과 **Subsampling**을 이용하고 있으며, Phrase (Ex. New York Times)를 기반으로 하여 기존 Word 기반의 문제점을 해결하는 **Phrase-based Skip-gram model**을 소개하고 있다.

--- 

### Skip-gram model

Skip-gram model은 Input 단어 주변의 단어를 예측하는 모델로, 다음과 같은 Average log probability를 최대화한다.

<img src="/assets/img/pexels/skipgram_obj.png" width="50%">

이때 Basic Skip-gram의 경우 $$p(w_{t+j} \vert w_t)$$를 아래와 같이 **softmax function**으로 계산한다.

<img src="/assets/img/pexels/skipgram_softmax.png" width="50%">

그러나 이와 같은 경우 미분 과정에서의 computational cost가 너무 높아 실사용이 어렵다.

이에 이전 Word2Vec 논문에서는 Output layer를 이진트리(Huffman tree)를 이용해 나타냄으로써 W번의 output nodes를 계산하지 않고 $$log_2(w)$$의 output nodes만을 계산하도록 하여 Computational cost를 크게 감소시켰다.

<img src="/assets/img/pexels/skipgram_tree.png" width="100%">

### Negative Sampling

본 논문에서는 Noise Contrastive Estimation(NCE)를 단순화한 방식인 Negative Sampling(NEG)를 이용하고 있다.

> **NCE** \\
 Logistic Regression을 이용하여 target data와 noise를 구분 \\
 -> softmax의 log probability를 최대화하도록 근사 \\
 -> Sample과 Noise 분포의 Numerical probability 필요

> **NEG** \\
 전체 단어 집합에 대해 target data와 noise data를 구분하는 logistic regression task를 수행하는 목적식을 K개 negative 단어를 이용하여 근사

이에 따른 목적식은 아래와 같다. 2번째 항을 통해 k개의 단어를 이용한 Negative Sampling을 반영하고 있다.

<img src="/assets/img/pexels/skipgram_neg.png" width="80%">


### Subsampling of Frequent Words

일부 과도하게 자주 등장하는 단어들은 단어 관계 학습에 중요한 정보를 전달해주지 않는다. 예를 들어 France/Paris와 달리 The/France의 경우 자주 함께 등장하나 이는 별로 의미가 없다. 따라서 이러한 단어 출현 빈도의 불균형을 해결하기 위해 Subsampling을 사용하고 있다.

Subsampling에서는 training set의 각 단어를 다음 식을 통해 계산된 확률에 따라 training set에서 제거한다.

<img src="/assets/img/pexels/skipgram_subsampling.png" width="40%">

이는 학습 속도의 향상 뿐만 아니라, rare words의 학습 정확도 역시 향상시킨다.

### Empirical Results

<img src="/assets/img/pexels/skipgram_results.png" width="90%">


### Phrase Skip-gram Results

일부 단어는 함께 등장하는 빈도가 개별로 등장하는 빈도에 비해 매우 높은 경우들이 존재한다. 이는 Phrase로 취급하여 하나의 token으로 해석해야 하며 이를 위해 Score를 계산한다.

아래 Score가 특정 threshold를 넘기면 phrase로 취급하며, threshold를 감소시키며 일정 횟수 반복한다.

<img src="/assets/img/pexels/skipgram_phrase.png" width="60%">

이를 통해 두 단어 또는 세 단어 등 여러 단어로 구성된 Phrase를 효과적으로 구별해낼 수 있다. 이에 대한 정확도는 72% 달성하였다고 한다.

### Additive Compositionality

논문의 저자들이 발견한 Word Embedding의 또 다른 특성은 Word Vector 간의 합 연산이 각 단어가 사용되는 **두 맥락적 분포의 Product**로 나타난다는 것이다. 이에 두 단어에 대해 모두 높은 확률을 가지고 있는 단어(And)를 얻을 수 있다.

---

> 효과적인 Word Embedding인 결국 더 큰 데이터를 다룰 수 있도록 하기에 매우 중요합니다. 기존의 Word2Vec을 이리저리 만져보고 Quality와 Speed 모두에서 더 나은 대안을 제시했다는 점이 인상깊습니다.
