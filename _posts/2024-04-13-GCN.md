---
layout: post
title: '[Paper Review] Semi-Supervised Classification with Graph Convolutional Networks' # Title of the page
hide_title: false # [Opt] Hide the title when displaying the post, but shown in lists of posts
#feature-img: "assets/img/sample.png"              # [Opt] Add a feature-image to the post
#thumbnail: "assets/img/thumbnails/sample-th.png"  # [Opt] Add a thumbnail image on blog view
#color: rgb(80,140,22)                             # [Opt] Add the specified colour as feature image, and change link colors in post
# position: 1 # [Opt] Set position on the menu navigation bar
tags: [GNN, Semi-Supervised Learning, Node Classification] # [Opt] Add tags to the page
# author:
categories: GNN
---

본 포스팅에서 다룰 논문은 'Semi-Supervised Classification with Graph Convolutional Networks'입니다. 2017년 ICLR에서 발표된 논문으로 Graph Neural Network 분야에서 Spectral Neural Network의 Spatial Neural Network로의 전환을 가져왔습니다. 이 부분에 대한 설명은 아래에서 서술하겠습니다. 

본 논문에서는 **Graph Network에 convolution 개념을 적용함으로써 Graph의 특성을 잘 반영한 machine learning model 구조**를 제시하고 **Node classification task**를 수행하고 있습니다.

*수식 유도가 주된 내용으로 구성되어 있으며 신호처리에 관한 내용이 포함되어 이해에 애를 먹었던 논문입니다. 여러 참고 자료를 기반으로 이해한 만큼 최대한 자세히 서술하고자 하였습니다.*

---

#### Reference
- *[고려대학교 산업경영공학부 DSBA 연구실](https://www.youtube.com/watch?v=F-JPKccMP7k)*

- *[Rala Sun Blog](https://ralasun.github.io/deep%20learning/2021/02/15/gcn/)*


---

## Abstract

We present a scalable approach for **semi-supervised learning** on graph-structured data that is based on **convolutional neural network**.

Our convolutional architecture was motivated by **localized first-order approximation** of spectral graph convolutions.

Our model **scales linearly** in the number of graph edges and learns hidden layer representations.

## Goals of the Paper

본 논문의 목표점은 graph에 직접적으로 적용시킬 수 있는 neural network layer-wise propagation rule을 찾고, 해당 방식이 spectral graph convolution의 first-order approximation으로 부터 도출되었음을 보이는 것에 있다.

또한 이렇게 얻어진 layer-wise propagation을 통해 fast & scalable한 semi-supervised node classification을 수행한다.

--- 

### Graph-based Semi-Supervised Learning

Graph 데이터의 경우, Label이 전체 데이터셋 대비 매우 적은 데이터에 대해서만 매겨져 있기 때문에 Regularization Loss term을 통해 전체 그래프로 smooth 시키는, 일종의 semi-supervised 방식을 사용한다.

이전의 방식은 일부 Label만을 이용하는 과정에서 인접 node는 동일한 label을 가짐을 가정한다. 그러나 이러한 접근 방식은 Edge가 similarity 이상의 정보를 담을 수 있다는 점을 반영하지 못하고 있기에 정보 손실이 발생한다.

따라서 여기서는 Regularization term을 사용하지 않는 대신 각 node의 feature를 convolution 연산을 통해 그래프 구조를 반영하여 hidden feature로 나타내고, hidden feature를 이용해 node classification 등의 task를 수행하고자 한다.

본 논문에서는 graph convolution을 푸리에 변환과 라플라스 행렬을 적용해 해결하고 있기에 이에 대한 배경 지식이 선행되어야 한다.

### Fast Approximate Convolutions on Graphs

본 논문에서 최종적으로 제시하고 있는 모델은 multi-layer Graph Convolutional Network (GCN)으로 아래와 같은 layer-wise propagation rule을 따른다.

<img src="/assets/img/pexels/GCN_prop.png" width="60%">

> $$\tilde{A} = A + I_N$$ : Adjacency matrix with self-connection added \\
$$\tilde{D}_{ii} = \sum_{j}{\tilde{A}_{ij}}$$ : Degree matrix of $$\tilde{A}$$ \\
$$W^{(l)}$$ : Layer-specific trainable weight matrix \\
$$\sigma(\cdot)$$ : Activation function (ex. ReLU) \\
$$H^{(0)} = X$$

위 propagation rule에서 $$\tilde{D}^{-1/2} \tilde{A} \tilde{D}^{-1/2}$$ 부분의 경우 Graph가 주어지면 단 1회 계산한다. 이를 $$\hat{A}$$으로 정의하고, node classification task에 적용한다면 model은 다음과 같이 간단하게 나타내어 질 수 있다.

<img src="/assets/img/pexels/GCN_simple_prop.png" width="70%">

본 섹션에서는 어떻게 이와 같은 layer-wise rule이 등장하였는지 motivation부터 설명하고자 한다.

#### Spectral Graph Convolution

Spectral Graph Convolution에 대응되는 개념으로 Spatial Graph Convolution 존재한다. 이 두 개념을 비교해보자면 다음과 같다.

**1. Spatial Graph Convolution**

이미지에서 Convolution filter가 이동하며 정보를 추출하는 것과 같이 Graph에서도 Convolution이 수행될 수 있다. 그러나 Graph는 고정된 위치가 아니라 위상적인 위치 관계만을 가지고 있기 때문에 convolution은 이웃 관계를 통해 이루어진다. 

Spatial graph convolution에서는 이웃 노드에서 정보를 받아 중심 노드의 정보를 업데이트 한다. 그러나 이러한 방식은 이웃 노드가 연쇄적으로 이어지기 때문에 시간이 지남에 따라 이웃 노드 말고도 멀리 있는 노드의 정보가 흘러들어오며 각 중심 노드에 연결된 이웃 노드의 수가 크게 차이나는 경우 일관적으로 정보 수합을 하기 어렵다는 단점이 있다.

그러나 아래 서술할 Spectral Convolution에 비해 계산 비용이 적고 개념이 직관적이라는 장점이 있다.

**2. Spectral Graph Convolution**

Spectral Graph Convolution에서는 각 노드의 정보를 signal로 보고 한 노드의 정보는 여러 노드들의 정보의 혼합으로 표현한다. 따라서 혼재된 signal을 분해하여 frequency domain으로 분석한다면 각 node의 특징을 더욱 잘 추출할 수 있을 것으로 본다. (분해 및 복원 과정에서 filtering 과정을 통해 noise를 제거하는 효과도 있다.)

Spatial Convolution에 비해 Spectral convolution이 더욱 계산 비용이 높고 개념이 복잡함에도 위 방식을 사용하는 경우가 꽤 있는데 이는 spectral convolution의 기반이 되는 fourier transform이 단단한 수학적 베이스를 가지고 있기 때문이다.

Spectral Graph Convolution을 이해하기 위해 알아야하는 배경 지식으로 Fourier Transform과 Laplace operator가 있다. 이를 먼저 설명하고 이것이 Graph Convolution으로 어떻게 이어지는 지 설명하고자 한다.

#### Fourier Transform

이미지 처리에서 사용하던 convolution을 graph에서 그대로 사용하지 못하는 이유는 graph는 Non-Euclidean 영역에서 정의되고 있기 때문이다. 따라서 graph domain에서의 convolution operator를 새롭게 정의해주어야 하는데 이때 필요한 것이 Fourier transform이다.

Spectral Graph Convolution에서 Fourier Transform이 등장하는 것은 Convolution Theorem을 이용하고 있기 때문이다. Convolution Theorem을 간단히 요약하자면 다음과 같다.

> $$F\{(f*g)(t)\} = F\{g(t)\} \odot F\{f(t)\}$$ \\
$$ \Leftrightarrow \{f*g\}(t) = F^{-1}\{F\{g(t)\} \odot F\{f(t)\}\}$$

위 수식이 나타내는 의미는 **"Convolution은 푸리에 도메인에서의 Point-wise Multiplication과 동일하다"**이다. Convolution 연산은 기본적으로 어떤 신호의 과거, 현재, 그리고 미래의 값들을 곱하고 합치는 연산을 의미한다. Graph Domain에서 과거, 현재, 미래란 각 노드가 직접 연결되는지, 한 노드를 거쳐 연결되는지, 두 노드를 거쳐 연결되는지 등으로 대치된다. (연결이 가까운 노드 = 비교적 최근 시점, 연결이 먼 노드 = 비교적 먼 시점)

이러한 Convolution 연산은 다양한 신호가 모일수록 복잡해지며 계산이 어려워진다. 그러나 Convolution Theorem에 따라 이를 푸리에 도메인으로 변환하고, 단순히 point-wise multiplication을 수행한다면 더욱 간편하게 계산이 가능하다.

그렇다면 푸리에 변환이란 무엇인가? 푸리에 변환은 Time domain에서 존재하는 신호(signal)을 Frequency domain으로 변환하는 것으로 다양한 주파수를 갖는 주기함수(sin,cos 등)의 선형결합으로 바꿔주는 과정이다. 이때 삼각함수의 경우 직교성을 갖기 때문에 푸리에 변환은 <u>선형대수 관점에서 살펴보면 signal을 sine과 cosine 기저들의 선형결합으로 나타내는 것</u>이라고 볼 수 있다. 

다시 Graph 도메인으로 돌아와서 생각해보면, Signal은 무엇이고 Frequency가 무엇인지 정의할 필요가 있다. Graph 도메인에서 Signal은 각 노드가 갖는 feature로, Frequency는 각 노드 feature 간의 차이로 대응시킬 수 있다.

즉, Graph Fourier Transform은 어떤 형태의 graph 관계가 signal에 어느정도 포함되어 있는지 알 수 있도록 하는 것으로 해석할 수 있다.


#### Laplace Operator

Laplace Operator는 Fourier Transform을 더 잘 이해하기 위해 꼭 알아두어야 할 부분이다. Laplace operator는 벡터 기울기의 발산(Divergence)을 의미한다. Divergence는  vector를 input으로 받아 scalar를 내놓는 연산으로 그 의미는 다음 그림을 통해 간단하게 이해할 수 있다.

<img src="/assets/img/pexels/GCN_divergence.png" width="80%">

즉 divergence는 특정 지점의 높고 낮음 또는 흐르는 정도를 의미한다고 할 수 있다. 

Laplace operator의 eigenfunction은 $$e^{2 \pi i\xi t}$$이며 이는 아래와 같이 푸리에 변환에서 등장한다.

<img src="/assets/img/pexels/GCN_fourier.png" width="70%">

즉, Laplace operator 측면에서 <u>푸리에 변환은 Laplace operator의 eigenfunction들의 합으로 분해하는 변환</u>으로 생각할 수 있다.

Graph 도메인에서도 Laplace operator를 정의할 수 있는데, 이를 Laplacian Matrix라고 부르며 Degree Matrix에서 Adjacency Matrix를 빼줌으로써 정의한다.

<img src="/assets/img/pexels/GCN_matrix.png" width="70%">

> Degree Matix: 대각행렬에 연결 정도 (Degree) 명시 \\
Adjacency Matrix: 비대각행렬에 인접 노드 명시

이렇게 만들어진 Laplacian Matrix의 가장 큰 특징은 <u>노드의 value(feature)를 나타내는 벡터와 Laplacian Matrix를 곱하면 한 노드와 이웃노드의 값의 차이를 구할 수 있다</u>는 것이다.

예를 들어 3개의 노드(feature는 각각 $$x_1, x_2, x_3$$)에 대해 Laplacian Matrix를 곱해주면 그 식은 다음과 같이 정리될 수 있다.

<img src="/assets/img/pexels/GCN_laplacianmatrix.png" width="70%">

즉 일반적인 Euclidian에서의 Laplace operator와 동일하게 Laplacian matrix 역시 "값의 차이"를 구하는 역할을 수행한다.


#### Spectral Graph Convolution

여기서는 위에서 설명했던 Fourier Transform과 Laplace operator가 Graph 상에서 어떤 의미를 가지며 실제적으로 Convolution이 계산될 때 어떻게 나타나는지 설명하고자 한다.

위에서 살펴보았다시피 Convolution 연산은 다음과 같은 식으로 나타내어진다.

$$
\{f*g\}(t) = F^{-1}\{F\{g(t)\} \odot F\{f(t)\}\}
$$

이에 기반해 Spectral Graph Convolution을 수식으로 나타내면 다음과 같은 관계를 갖는다.

<img src="/assets/img/pexels/GCN_convolution_decomposition.png" width="90%">

어떤 신호에 대해 Graph Convolution을 수행하는 것은 신호에 Laplacian Matrix를 곱해주는 것과 동일하다. 이 Laplacian Matrix를 Eigen vector decomposition한 뒤 해당 결과를 순차적으로 살펴보면 (1) Graph Fourier Transform (2) Eigen value filtering (3) Inverse Graph Fourier Transform이 수행된 것을 알 수 있다. *PCA와 유사하다.*

Graph Convolution에서는 Graph Fourier Transform을 통해 Frequency가 낮은 관계들을 우선적으로 반영하고자 한다. Frequency가 낮다는 것은 Node간 차이가 작은 것이며, 이는 근거리 Node 간의 관계를 더욱 크게 반영하겠다는 것이다.

이를 위해 Graph Fourier Transform에서는 Eigen value를 오름차순으로 정렬하여 Eigen value가 작은 관계들을 우선적으로 반영한다.

### GCN

그러나 이러한 방식을 그대로 사용하기에는 Eigen Decomposition으로 인한 Computational cost가 과도하게 높다. 또한 전체 Eigen Value를 모두 사용하는 경우 Localizing이 되지 않는다.

따라서 본 논문에서는 Localized Filter로 만들기 위해 Polynomial Parameterization을 시행하고, Chebyshev Polynomials를 이용해 다항식을 근사하였다.

그 과정은 다음과 같다.

1. Polynomial Parameterization ($$k$$ localized)

$$
g'_{\theta}(\Lambda) = \sum^{K}_{k=0} \theta^{'}_k \Lambda^{k}
$$

$$
U(\sum^{K}_{k=0} \theta^{'}_k \Lambda^{k})U^Tx = \sum^{K}_{k=0} \theta^{'}_k U \Lambda^{k} U^{T} x 
= \sum^{K}_{k=0} \theta^{'}_k L^k x
$$

2. ChebyShev Polynomials ($$T_k (x)$$)

$$
g'_{\theta}*x \approx \sum^{K}_{k=0} \theta^{'}_k T_k(\tilde{L}) x
$$

이를 Layer-wise하게 정의하기 위해 K는 1로 설정하고 식을 정리하면 다음과 같다.

$$
g'_{\theta}*x \approx \theta^{'}_0 T_0(\tilde{L}) x + \theta^{'}_1 T_1(\tilde{L}) x
$$

$$
\Leftrightarrow \theta (I_N+D^{-1/2}AD^{-1/2})x
$$

식을 더욱 간단히 하기 위해 Renormalization을 수행하고 이를 Neural Network 형식으로 표기하면 다음과 같다.

$$
Z=\tilde{D}^{-1/2} \tilde{A} \tilde{D}^{-1/2} X \Theta
$$

<img src="/assets/img/pexels/GCN_simple_prop.png" width="70%">

위 식에서 $$\hat{A}$$는 $$\tilde{D}^{-1/2} \tilde{A} \tilde{D}^{-1/2}$$이다.

### Semi-Supervised Node Classification

위 과정을 통해 도출된 GCN 모델을 Labeled data를 이용해 학습시키고, Unlabed data에 대해 Node Classification을 수행한 결과는 다음과 같이 나타났다.

<img src="/assets/img/pexels/GCN_results.png" width="80%">

---

> 본 논문은 기존까지 Graph 구조의 독특한 특성 때문에 Spectral based approach가 보편적이었던 것을 Approximation 과정을 통해 Spatial based approach가 더욱 활발해질 수 있도록 전환시켰다는 점에서 큰 의의를 가지고 있습니다. 여러가지 트릭을 사용함으로써 적절한 수식을 유도해내는 과정이 인상깊었습니다.
