---
layout: post
title: '[Paper Review] DeepWalk: Online Learning of Social Representations' # Title of the page
hide_title: false # [Opt] Hide the title when displaying the post, but shown in lists of posts
#feature-img: "assets/img/sample.png"              # [Opt] Add a feature-image to the post
#thumbnail: "assets/img/thumbnails/sample-th.png"  # [Opt] Add a thumbnail image on blog view
#color: rgb(80,140,22)                             # [Opt] Add the specified colour as feature image, and change link colors in post
# position: 1 # [Opt] Set position on the menu navigation bar
tags: [GNN, Node Embedding, Node Classification] # [Opt] Add tags to the page
# author:
categories: GNN
---

본 포스팅에서 리뷰할 논문은 'DeepWalk: Online Learning of Social Representations'입니다. 2014년 발표된 논문으로 Graph의 각 node를 효과적으로 embedding하여 각 노드에 대한 feature vector를 도출하고자 하고 있습니다. 

본 논문에서는 **Graph Network** 구조에서 Sequential한 정보를 뽑아내고 **NLP**에서 사용되던 Vector Embedding 모델을 적용시키고 있습니다.

---

## Abstract

We present **DeepWalk**, a novel approach for **learning latent representations of vertices** in a network. 

DeepWalk generalizes recent advancements in **language modeling** and **unsupervised feature learning**.

We demonstrate DeepWalk's latent representations on several **multi-label network classification** tasks for social networks such as Blog Catalog, Flickr, and YouTube.

## Goals of the Paper

본 논문에서는 Statistical modeling에 적합한 Robust한 latent representation을 형성하고자 하며 이를 통해 기존 다른 모델들보다 더욱 높은 성능을 달성하고자 한다.

이때 모델이 병렬적으로 학습할 수 있고, 쉽게 update될 수 있도록하여 scalability를 확보하고자 한다.

--- 
### Learning Social Representation

저자들은 Scalability와 Continual Learning이 가능한 모델을 구축하고자 하였다. 아래는 저자들이 Social Representation이 가져야할 특성으로 정리한 내용이다.

- **Adaptability**: 실제 social network는 계속해서 진화하기 때문에 새로운 social relation이 발생했을 때 전체 프로세스를 다시 수행하지 않고도 이를 반영할 수 있어야 한다.
- **Community aware**: Latent dimension 간의 거리는 social similarity를 나타낼 수 있어야 한다.
- **Low dimensional**: 라벨이 있는 데이터가 희소할 때 저차원의 모델은 더 잘 일반화되며, 빠르게 수렴하고, 추론 가능하다.
- **Continuous**: Continuous representation은 smooth decision boundaries를 가지고 있어 더욱 robust한 분류가 가능하다.

### DeepWalk

본 논문에서는 graph 구조 속 각 노드의 연결을 반영하여 각 노드를 continuous한 낮은 차원의 벡터공간에 나타내는 노드 임베딩 방법론으로써 DeepWalk를 제안하고 있다. 

DeepWalk는 NLP 분야에서 제안된 단어 임베딩 방법론인  Word2Vec의 Skip-gram을 이용하는데, 이때 문장은 *Random walk* sequence로 대응되고 단어는 각 노드로 대응된다.

#### Random Walk

> **Random Walk** \\
그래프 구조를 Sequential한 정보로 전환하기 위해 사용하는 대표적인 방법으로, 어떠한 node에서 시작해서 연결된 node로 이동하는 것으로 이때 만들어지는 Sequence를 나열한다. \\

<img src="/assets/img/pexels/DeepWalk_randomwalk.png" width="80%">


Random Walk rooted at vertex $$v_{i}$$: $$W_{v_{i}}$$

Random Walk는 similarity를 측정하기 위한 용도로 사용되어왔다. 또한, *Stream* of short random walk를 이용하면 Graph에서 local한 정보를 추출해낼 수 있다. 이를 이용함으로써 얻을 수 있는 주요 효과로는, 쉬운 parallelize와 graph의 작은 변화에 대한 쉬운 accomodation이 있다.

#### Connection (Graph-Text): Power laws

만약 graph의 degree distribution이 power-law를 따른다면, short random walk에서 각 노드의 관측 빈도 역시 power-law를 따를 것이다. 이때, NLP에서 각 단어의 발생 빈도 역시 power-law를 따르기에 이런 두 유사성을 바탕으로 NLP 분야에서의 techniques를 차용하여 사용할 수 있을 것이라 예상할 수 있다.

#### Language Modeling

DeepWalk는 NLP 분야에서 제안된 단어 임베딩 방법론인  Word2Vec의 Skip-gram을 이용하는데, 이때 문장은 Random walk sequence로 대응되고 단어는 각 노드로 대응된다. Mapping function은 다음과 같이 표현한다.

$$
\Phi: v \in V \to R^{|V|\times d}
$$

이를 통해 optimization 문제를 정의한 결과는 다음과 같다. 이를 통해 각 노드의 vector representation을 얻을 수 있다.

$$
minimize_{\Phi} -log Pr({v_{i-w}, ..., v_{i-1}, v_{i+1},...,v_{i+w}}|\Phi(v_{i}))
$$

이때 partition function을 계산하는 것이 높은 computational cost를 요하기 때문에 이를 binary tree에 할당한 후 계산함으로써 계산복잡도를 낮출 수 있다. 이를 위해 기존 skip-gram 알고리즘에서 사용하였던 Hierarchical Softmax를 추가적으로 이용하였다.

### Results

본 논문에서는 Blog Catalog, Flickr, YouTube 세 데이터셋을 이용하여 모델을 평가하고 비교하였다. 그 결과는 아래와 같다.

<img src="/assets/img/pexels/DeepWalk_result1.png" width="90%">

<img src="/assets/img/pexels/DeepWalk_result2.png" width="90%">

<img src="/assets/img/pexels/DeepWalk_result3.png" width="90%">

기존 모델 대비 성능 향상을 이끌어냈음을 확인할 수 있다.

---

> 본 논문에서는 상당히 간단하게 Random Walk를 적용함으로써 Graph의 Node Representation을 수행하고 있다. 그러나 이 과정에서 정보의 손실 및 더욱 다양한 연결관계를 반영하지 못한다는 문제가 있다. Random Walk를 이용하는 후속 논문을 다뤄보면서 어떤 문제점을 어떻게 해결했는지 살펴볼 예정이다.
